{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               author        date  year month         topic  \\\n",
       "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "\n",
       "                                             article  \n",
       "0  PARIS  ?   When the Islamic State was about to...  \n",
       "1  Angels are everywhere in the Mu?iz family?s ap...  \n",
       "2  Finally. The Second Avenue subway opened in Ne...  \n",
       "3  WASHINGTON  ?   It?s   or   time for Republica...  \n",
       "4  For Megyn Kelly, the shift from Fox News to NB...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('news_dataset.csv', encoding='latin1')\n",
    "\n",
    "(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       1000 non-null   int64 \n",
      " 1   author   994 non-null    object\n",
      " 2   date     1000 non-null   object\n",
      " 3   year     1000 non-null   object\n",
      " 4   month    1000 non-null   object\n",
      " 5   topic    1000 non-null   object\n",
      " 6   article  1000 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 54.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4') \n",
    "\n",
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean(text):\n",
    "   \n",
    "    text = text.replace('?', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['article'] = df['article'].apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text for TF-IDF calculation\n",
    "def preprocess_text(text):\n",
    "    # Tokenize into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalnum()]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess articles and store both original and preprocessed versions\n",
    "df['sentences'] = df['article'].apply(sent_tokenize)\n",
    "df['pre-processed_sentences'] = df['sentences'].apply(lambda sentences: [preprocess_text(sentence) for sentence in sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>pre-processed_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS When the Islamic State was about to be d...</td>\n",
       "      <td>[PARIS When the Islamic State was about to be ...</td>\n",
       "      <td>[paris islamic state driven ancient city palmy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu iz family s ap...</td>\n",
       "      <td>[Angels are everywhere in the Mu iz family s a...</td>\n",
       "      <td>[angel everywhere mu iz family apartment bronx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "      <td>[Finally., The Second Avenue subway opened in ...</td>\n",
       "      <td>[finally, second avenue subway opened new york...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON It s or time for Republicans. After...</td>\n",
       "      <td>[WASHINGTON It s or time for Republicans., Aft...</td>\n",
       "      <td>[washington time republican, tumultuous decade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "      <td>[For Megyn Kelly, the shift from Fox News to N...</td>\n",
       "      <td>[megyn kelly shift fox news nbc host daily day...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               author        date  year month         topic  \\\n",
       "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "\n",
       "                                             article  \\\n",
       "0  PARIS When the Islamic State was about to be d...   \n",
       "1  Angels are everywhere in the Mu iz family s ap...   \n",
       "2  Finally. The Second Avenue subway opened in Ne...   \n",
       "3  WASHINGTON It s or time for Republicans. After...   \n",
       "4  For Megyn Kelly, the shift from Fox News to NB...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [PARIS When the Islamic State was about to be ...   \n",
       "1  [Angels are everywhere in the Mu iz family s a...   \n",
       "2  [Finally., The Second Avenue subway opened in ...   \n",
       "3  [WASHINGTON It s or time for Republicans., Aft...   \n",
       "4  [For Megyn Kelly, the shift from Fox News to N...   \n",
       "\n",
       "                             pre-processed_sentences  \n",
       "0  [paris islamic state driven ancient city palmy...  \n",
       "1  [angel everywhere mu iz family apartment bronx...  \n",
       "2  [finally, second avenue subway opened new york...  \n",
       "3  [washington time republican, tumultuous decade...  \n",
       "4  [megyn kelly shift fox news nbc host daily day...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: []\n",
      "Explanation: A Republican in the White House and a Republican majority in Congress present tremendous opportunity to make real progress, Senator Cory Gardner, Republican of Colorado, said in the party s weekly radio address on Saturday.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add neuralcoref to spaCy's pipe\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# Function to apply coreference resolution\n",
    "def resolve_coreferences(text):\n",
    "    doc = nlp(text)\n",
    "    return doc._.coref_clusters,doc._.coref_resolved\n",
    "\n",
    "# Apply coreference resolution to the article texts\n",
    "information_cluster,short_explaination= resolve_coreferences(sentence)\n",
    "\n",
    "print(\"cluster:\",information_cluster)\n",
    "print(\"Explanation:\",short_explaination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_sentence(question, article_index):\n",
    "    # Check if the article index is valid and if data exists for that index\n",
    "    if article_index not in df.index:\n",
    "        return \"No article found\", 0\n",
    "\n",
    "    # Get processed sentences and original sentences for the specific article\n",
    "    pre_processed_sentences = df.loc[article_index, 'pre-processed_sentences']\n",
    "    original_raw_sentences = df.loc[article_index, 'sentences']\n",
    "\n",
    "    if not pre_processed_sentences:\n",
    "        return \"No valid sentences in article\", 0\n",
    "    \n",
    "    # Include the question in the processing for vectorization\n",
    "    processed_question = preprocess_text(question)\n",
    "    processed_sentences_with_question = pre_processed_sentences + [processed_question]\n",
    "\n",
    "    # Vectorize the sentences including the question\n",
    "    sentence_vectors = tfidf_vectorizer.fit_transform(processed_sentences_with_question)\n",
    "\n",
    "    # Calculate cosine similarities between the question and each processed sentence\n",
    "    similarities = cosine_similarity(sentence_vectors[-1], sentence_vectors[:-1]).flatten()\n",
    "\n",
    "    # Find the index of the highest similarity score\n",
    "    if len(similarities) == 0:\n",
    "        return \"No similarities found\", 0\n",
    "\n",
    "    most_relevant_idx = similarities.argmax()\n",
    "\n",
    "    # Check if the index is within the range of original sentences\n",
    "    if most_relevant_idx >= len(original_raw_sentences):\n",
    "        return \"No relevant sentence found\", 0\n",
    "\n",
    "    return original_raw_sentences[most_relevant_idx], similarities[most_relevant_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant sentence: A Republican in the White House and a Republican majority in Congress present tremendous opportunity to make real progress, Senator Cory Gardner, Republican of Colorado, said in the party s weekly radio address on Saturday.\n",
      "Confidence score: 0.30232007325256094\n"
     ]
    }
   ],
   "source": [
    "article_index = df[df['id'] == 17311].index[0]  # Adjust based on how the index is set in your DataFrame\n",
    "question = \"Who is the Senator of Colorado?\"\n",
    "sentence, confidence = find_relevant_sentence(question, article_index)\n",
    "print(\"Most relevant sentence:\", sentence)\n",
    "print(\"Confidence score:\", confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Load the large model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def question_focus_type(question):\n",
    "    doc = nlp(question)\n",
    "    question_word = next((tok for tok in doc if tok.dep_ == 'ROOT' or tok.head.dep_ == 'ROOT'), None)\n",
    "    focus_words = {\"who\": [\"PERSON\", \"ORG\"], \"where\": [\"GPE\", \"LOC\"], \"when\": [\"DATE\", \"TIME\"]}\n",
    "    \n",
    "    if question_word:\n",
    "        for key, types in focus_words.items():\n",
    "            if key in question.lower():\n",
    "                return types\n",
    "    return [\"ORG\", \"PERSON\", \"GPE\", \"LOC\", \"DATE\", \"TIME\", \"EVENT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Answer: Cory Gardner\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def correct_answer(question, sentence):\n",
    "    expected_types = question_focus_type(question)\n",
    "    doc = nlp(sentence)\n",
    "    relevant_entities = [ent for ent in doc.ents if ent.label_ in expected_types]\n",
    "    \n",
    "    # Improved scoring by checking the proximity to the root or question words\n",
    "    best_score = float('-inf')\n",
    "    best_entity = None\n",
    "    for entity in relevant_entities:\n",
    "        # Score entities based on proximity to the root or important tokens\n",
    "        entity_score = 100 - min(abs(tok.i - entity.root.i) for tok in doc if tok.dep_ == 'ROOT' or tok.head.dep_ == 'ROOT')\n",
    "        if entity_score > best_score:\n",
    "            best_score = entity_score\n",
    "            best_entity = entity.text\n",
    "    \n",
    "    return best_entity if best_entity else sentence, best_score if best_score != float('-inf') else 0\n",
    "best_answer, confidence = correct_answer(question, sentence)\n",
    "print(f\"Best Answer: {best_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('test_questions.csv')  # Assuming columns: ['article_id', 'question', 'correct_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    # Assuming text is a string of words separated by spaces\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_calculation(gold, pred):\n",
    "    # print(\"original\",gold)\n",
    "    # print(\"Predicted\",pred)\n",
    "    gold_toks = tokens(gold)\n",
    "    pred_toks = tokens(pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate this function into the evaluation system\n",
    "def evaluate_f1(test_data):\n",
    "    f1_scores = []\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        article_ids = df[df['id'] == row['Article ID']].index\n",
    "        if article_ids.empty:\n",
    "            f1_scores.append(0)\n",
    "            continue  # Skip if the article ID is not found\n",
    "        article_index = article_ids[0]\n",
    "\n",
    "        retrieved_sentence, _ = find_relevant_sentence(row['Question'], article_index)\n",
    "        predicted_answer, _ = correct_answer(row['Question'], retrieved_sentence)\n",
    "        correct_A_answer = row['Answer']\n",
    "\n",
    "        f1 = f1_score_calculation(correct_A_answer, predicted_answer)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate the average F1 score across all test samples\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 16.75%\n"
     ]
    }
   ],
   "source": [
    "# Now, call evaluate_f1 with test data\n",
    "average_f1 = evaluate_f1(test_data)\n",
    "print(f\"Average F1 Score: {average_f1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Cory Gardner\n"
     ]
    }
   ],
   "source": [
    "def question_answer():\n",
    "    article_id = int(input(\"Enter article ID: \"))\n",
    "    question = input(\"Enter your question: \")\n",
    "    try:\n",
    "        # Find the index of the article with the given ID\n",
    "        article_index = df.index[df['id'] == article_id]\n",
    "        if not article_index.empty:\n",
    "            # Get the first index if there are multiple articles with the same ID\n",
    "            article_index = article_index[0]\n",
    "            # Assuming 'find_relevant_sentence' is already defined and working correctly\n",
    "            sentence, confidence = find_relevant_sentence(question, article_index)  # Pass the article index directly\n",
    "           \n",
    "            \n",
    "            # Assuming 'correct_answer' is already defined and working correctly\n",
    "            best_answer, confidence = correct_answer(question, sentence)\n",
    "            print(\"Answer:\", best_answer)\n",
    "        else:\n",
    "            print(\"No article found with the provided ID.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error processing your request:\", str(e))\n",
    "\n",
    "question_answer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
